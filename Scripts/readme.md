ğŸ“Š Media Stream Analytics â€“ Real-Time Intelligence for Media Decisions

## ğŸ§  Project Overview
This project enables real-time and batch data analytics for the media industry by integrating AWS cloud-native services with Apache Spark. It helps media companies:

- Analyze viewership behavior  
- Optimize ad revenue strategies  
- Understand demographic trends  
- Automate data pipelines using Airflow and Lambda  

---

## ğŸ—ï¸ Architecture & Technologies Used

### ğŸ”§ AWS Services
- **Amazon S3** â€“ Data lake and intermediate storage  
- **AWS Glue** â€“ ETL (Transform, Load) and Crawlers  
- **AWS Lambda** â€“ Triggers for real-time orchestration  
- **Amazon EMR** â€“ Spark-based batch & stream processing  
- **Amazon Kinesis** â€“ Real-time data ingestion  
- **Amazon Athena** â€“ Serverless interactive SQL analytics  
- **Amazon MWAA (Airflow)** â€“ DAG-based orchestration  

### âš™ï¸ Processing & Query Engines
- Apache Spark (batch & structured streaming)  
- Iceberg Table Format  
- Snowflake (optional for warehouse queries)  
- Athena SQL (for querying Iceberg/Parquet)  

### ğŸ“¦ Data Formats
- CSV â€“ Raw source files  
- Parquet â€“ Optimized storage  
- Iceberg â€“ ACID-compliant modern table format  
- JSON â€“ For real-time streaming  

---

## ğŸš€ Project Workflow

### ğŸ“Œ Phase 1: Data Generation (Local)
Generate synthetic media datasets.

```bash
vi generate_media_data.py
python3 generate_media_data.py
```

Files generated:
- `channel_metadata.csv`
- `demographics.csv`
- `ad_revenue.csv`
- `viewership_logs.csv`

---

### ğŸ“Œ Phase 2: Upload Raw Files to S3
```bash
aws s3 cp *.csv s3://aahash-project2/raw/
```

---

### ğŸ“Œ Phase 3: Catalog with AWS Glue Crawlers
Create and run crawlers for:
- `ad_revenue`
- `channel_metadata`
- `demographics`

Verify table creation in Glue database: `media_raw_aahash`

---

### ğŸ“Œ Phase 4: Transform and Store as Iceberg
Create Spark ETL jobs using AWS Glue Studio or submit jobs to EMR.

Example target paths:
- `s3://aahash-project2/media/iceberg/ad_revenue/`
- `s3://aahash-project2/media/iceberg/channel_metadata/`
- `s3://aahash-project2/media/iceberg/demographics/`

---

### ğŸ“Œ Phase 5: Lambda & MWAA (Airflow) Integration
- **Lambda Function**: Triggers MWAA DAG when new Iceberg files are added.  
- **Airflow DAG**: Executes Glue job for incremental merge.

```python
# Lambda Function
TriggerMWAADAGOnIcebergUpdate

# Airflow DAG ID
trigger_glue_merge_dag
```

---

### ğŸ“Œ Phase 6: Real-Time Log Streaming with Kinesis

Send Logs to Kinesis:
```bash
python3 send_to_kinesis.py
```

Process Streaming Data on EMR:
```bash
spark-submit spark_to_s3_snowflake.py
```

Ensure Spark script writes to:
- S3 â†’ `s3://aahash-project2/stream1/`
- (Optionally) Snowflake

---

### ğŸ“Œ Phase 7: Query Streamed Data Using Athena
Use Glue Crawler to crawl the S3 path: `s3://aahash-project2/stream1/`

Query via Athena using prebuilt SQL queries.

Example query:
```sql
SELECT region, COUNT(*) AS views 
FROM viewership_stream 
GROUP BY region;
```

---

## ğŸ“ Folder Structure
```bash
.
â”œâ”€â”€ generate_media_data.py
â”œâ”€â”€ send_to_kinesis.py
â”œâ”€â”€ spark_to_s3_snowflake.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ [supporting transformation scripts]
â”œâ”€â”€ historical_data/
â”œâ”€â”€ s3://aahash-project2/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ media/iceberg/
â”‚   â”œâ”€â”€ stream1/
```

---

## âœ… Prerequisites
- AWS CLI configured  
- AWS EMR cluster or AWS Glue jobs  
- IAM roles with access to:
  - S3  
  - Kinesis  
  - Glue  
  - Lambda  
  - Athena  
- Python 3.x  
- Apache Spark 3.x compatible setup  
```
